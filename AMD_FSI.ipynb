{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"0KiCokcnK1Po"},"source":["# **AMD Project: Finding Similar Items**\n","\n","*   **Author:** Matteo Onger\n","*   **Date:** July 2023\n","\n","The following code implements the MinHash-LSH technique to find similar items.\\\n","The code is organized in three main classes: one to compute characteristic vectors, one to compute signatures and one to apply the LSH technique. Six global variables allow the customisation of the algorithm and are passed as parameters to the constructors of these classes. Four functions (*words*, *nonStopWords*, *joinStopWords* and *kgrams*) have already been defined to compute the shingles in four different ways and another function is used to preprocess the raw data.\\\n","The dataset is organized as a Spark RDD, so its standard methods are used to apply the previously defined functions/methods to the entire dataset. Of course, this approach guarantees a better scalability."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4yK4Vph6LFEY"},"source":["## Main Code\n","---\n","**Note:** It is possible to edit the parameters to customize the execution of the algorithm (7th cell).\\\n","**Note:** KAGGLE_USERNAME and KAGGLE_KEY must be entered to download the dataset (8th cell).\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9686,"status":"ok","timestamp":1687781524714,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"rwGa3QJ1W-i2","outputId":"896f0c6c-3727-410b-c125-2e27c0b2bff9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["# ---- INSTALL LIBRARIES ----\n","!pip install pyspark"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2966,"status":"ok","timestamp":1687781527678,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"MjP2Wd2JojWa"},"outputs":[],"source":["# ---- LIBRARIES ----\n","import json\n","import nltk\n","import numpy as np\n","import os\n","import pyspark\n","import pyspark.sql.types as tp\n","import random\n","import re\n","import sympy\n","import time\n","\n","from collections.abc import Callable\n","from operator import add\n","from os.path import isfile\n","from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":508,"status":"ok","timestamp":1687781528183,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"BFk2mOrHabKQ"},"outputs":[],"source":["# ---- PREPROCESSING FUNCS ----\n","def preprocessing(text :str) -> str:\n","  \"\"\"\n","  Return the string obtained by replacing all the non-alphanumeric characters\n","  of ``text`` with a whitespace. All the letters are made lowercase and multiple\n","  consecutive whitespaces are collapsed into a single whitespace.\n","  \"\"\"\n","  text = re.sub('[^A-Za-z0-9 ]+', ' ', text.lower())\n","  text = re.sub('\\s{2,}', ' ', text)\n","  return text\n","\n","\n","# ---- SHINGLES FUNCS ----\n","def words(text :str) -> set[str]:\n","  \"\"\"\n","  Return a set of shingles, which, in this case, are words.\n","  So ``text`` is split according to whitespaces.\n","  \"\"\"\n","  res = set(text.split(' '))\n","  res.discard('')\n","  return res\n","\n","\n","def nonStopWords(text :str) -> set[str]:\n","  \"\"\"\n","  Return a set of shingles, which, in this case, are words, but excluding stopwords.\n","  ``stopWordsEn`` is a set and must be a global variable containing all the desired stopwords.\n","  \"\"\"\n","  res = set(text.split(' '))\n","  res.discard('')\n","  res.difference_update(stopWordsEn)\n","  return res\n","\n","\n","def joinStopWords(text :str) -> set[str]:\n","  \"\"\"\n","  Return a set of shingles, which, in this case, are words with the stopwords joined to the next two words.\n","  ``stopWordsEn`` is a set and must be a global variable containing all desired stopwords.\n","  \"\"\"\n","  res = set()\n","  text = text.split(' ')\n","  for w in range(0, len(text)):\n","    if w == '':\n","      continue\n","    elif text[w] in stopWordsEn:\n","      res.add(text[w] + (text[w+1] if w+1<len(text) else '') + (text[w+2] if w+2<len(text) else ''))\n","      w += 3\n","    else:\n","      res.add(text[w])\n","  return res\n","\n","\n","def kgrams(text :str) -> set[str]:\n","  \"\"\"\n","  Return a set of shingles, which, in this case, are K-grams.\n","   ``K`` is a global variable.\n","  \"\"\"\n","  if len(text) < K:\n","    text += (K-len(text)) * ' '\n","  grams = set()\n","  for i in range(0, len(text)-K+1):\n","    grams.add(text[i:i+K])\n","  return grams"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1687781528184,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"lVS4rui5nJVg"},"outputs":[],"source":["# ---- CHARACTERISTIC MATRIX CLASS ----\n","class CharacteristicMatrix:\n","  \"\"\"\n","  The following class implements methods to compute characteristic vectors\n","  and the jaccard index.\n","  \"\"\"\n","\n","  def __init__(self, maxNumShingles :int, computeShingles :Callable[[str], set[str]], verbose :bool=False) -> None:\n","    \"\"\"\n","    Create a new object to compute characteristic vectors.\n","    Shingles will be computed using the function ``computeShingles`` and\n","    they will be stored as an integer between [0,``maxNumShingles``), so\n","    if the number of distinct shingles is greater or equal than ``maxNumShingles``,\n","    some will be associated with the same index (collisions).\n","    \"\"\"\n","    self.maxNumShingles = maxNumShingles\n","    self._computeShingles = computeShingles\n","    self._verb = verbose\n","\n","    if self._verb:\n","      print('New characteristic matrix created:')\n","      print(' - Max num shingles -> ' + str(self.maxNumShingles) + \"\\n\")\n","    return\n","\n","\n","  def computeCharVect(self, doc :str, key :str='') -> dict[str, str|set[int]]:\n","    \"\"\"\n","    Compute the characteristic vector of the document ``doc``.\n","\n","    Parameters\n","    ----------\n","    ``key``: string, optional\n","      ID of the passed document ``doc``. Empty string as default value.\n","    ``doc``: string\n","      Text to analyze.\n","\n","    Return\n","    ----------\n","    Return a dictionary ``y`` that contains the key (unchanged) of the given document and\n","    a set ``s`` of integer: they represent the shingles of ``doc``.\n","    ``y``: dict\n","      {'key':``key``, 'shin':``s``}\n","    \"\"\"\n","    shingles = set()\n","    for s in self._computeShingles(doc):\n","      idx = hash(s) % self.maxNumShingles\n","      shingles.add(idx)\n","    return {'key':key, 'shin':shingles}\n","\n","\n","  def js(self, a :dict[str, str|set[int]], b :dict[str, str|set[int]]) -> dict[str, str|float]:\n","    \"\"\"\n","    Compute the jaccard similarity coefficient of the docs ``a`` and ``b``, given their characteristic vectors.\n","\n","    Parameters\n","    ----------\n","    ``a``, ``b``: dict\n","      The field 'shin' must contain the characteristic vector,\n","      while the field 'key' (optional) can contain the doc's id.\n","      If this field doesn't exist, an empty string is used as key.\n","\n","    Return\n","    ----------\n","    Return a dictionary ``y`` that contains the keys (unchanged) of the given documents and\n","    a float ``f`` that is the jaccard index.\n","    ``y``: dict\n","      {'keyA':``a['key']``, 'keyB':``b['key']``, 'JS':``f``}\n","    \"\"\"\n","    keyA = a['key'] if 'key' in a else ''\n","    keyB = b['key'] if 'key' in b else ''\n","    a = a['shin']\n","    b = b['shin']\n","\n","    return {'keyA':keyA, 'keyB':keyB, 'JS':len(a.intersection(b)) / len(a.union(b))}"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1687781528184,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"vlY0Sm_FnLF6"},"outputs":[],"source":["# ---- SIGNATURE MATRIX CLASS ----\n","class SignatureMatrix:\n","  \"\"\"\n","  The following class implements methods to compute signatures\n","  and the estimated jaccard index.\n","  \"\"\"\n","\n","  def __init__(self, numMinHashes :int, charMat :CharacteristicMatrix, verbose :bool=False) -> None:\n","    \"\"\"\n","    Create a new object that computes ``numMinHashes`` MinHashes per docs\n","    using ``charMat`` to get the characteristic vector of all the docs treated.\n","    \"\"\"\n","    self.nMH = numMinHashes\n","    self._charMat = charMat\n","    self._seedsA = np.array([random.randint(0, self._charMat.maxNumShingles) for i in range(0, self.nMH)])\n","    self._seedsB = np.array([random.randint(0, self._charMat.maxNumShingles) for i in range(0, self.nMH)])\n","    self._seedsC = np.array([sympy.randprime(self._charMat.maxNumShingles, 2*self._charMat.maxNumShingles) for i in range(0, self.nMH)])\n","    self._verb = verbose\n","\n","    if self._verb:\n","      print('New signature matrix created:')\n","      print(' - Num MinHashes -> ' + str(self.nMH))\n","      print(' - SeedsA -> ' + str(self._seedsA))\n","      print(' - SeedsB -> ' + str(self._seedsB))\n","      print(' - SeedsC -> ' + str(self._seedsC) + \"\\n\")\n","    return\n","\n","\n","  def computeSignature(self, doc :str, key :str='') -> dict[str, str|tuple[int]]:\n","    \"\"\"\n","    Compute the signature of the document ``doc``.\n","\n","    Parameters\n","    ----------\n","    ``key``: string, optional\n","      ID of the passed document ``doc``. Empty string as default value.\n","    ``doc``: string\n","      Text to analyze.\n","\n","    Return\n","    ----------\n","    Return a dictionary ``y`` that contains the key (unchanged) of the given document and\n","    a tuple ``t`` of integer: it is the signature of ``doc``.\n","    ``y``: dict\n","      {'key':``key``, 'sign':``t``}\n","    \"\"\"\n","    shingles = self._charMat.computeCharVect(doc)['shin']\n","    signature = np.full(self.nMH, self._charMat.maxNumShingles)\n","    # Row index in the original char mat\n","    for idx in shingles:\n","      # Row indexes in the simulated permutations\n","      hashI = np.mod(np.mod(np.add(idx * self._seedsA, self._seedsB), self._seedsC), self._charMat.maxNumShingles)\n","      np.minimum(signature, hashI, out=signature)\n","    return {'key':key, 'sign':tuple(signature)}\n","\n","\n","  def ejs(self, a :dict[str, str|set[int]], b :dict[str, str|set[int]]) -> dict[str, str|float]:\n","    \"\"\"\n","    Compute the estimated jaccard similarity coefficient of the docs ``a`` and ``b``, given their signatures.\n","\n","    Parameters\n","    ----------\n","    ``a``, ``b``: dict\n","      The field 'sign' must contain the signature,\n","      while the field 'key' (optional) can contain the doc's id.\n","      If this field doesn't exist, an empty string is used as key.\n","\n","    Return\n","    ----------\n","    Return a dictionary ``y`` that contains the keys (unchanged) of the given documents and\n","    a float ``f`` that is the estimated jaccard index.\n","    ``y``: dict\n","      {'keyA':``a['key']``, 'keyB':``b['key']``, 'EJS':``f``}\n","    \"\"\"\n","    keyA = a['key'] if 'key' in a else ''\n","    keyB = b['key'] if 'key' in b else ''\n","    a = a['sign']\n","    b = b['sign']\n","    return {'keyA':keyA,  'keyB':keyB, 'EJS':sum([a[i] == b[i] for i in range(0, len(a))]) / len(a)}"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1687781528184,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"heuqM8MS2w4v"},"outputs":[],"source":["# ---- LSH CLASS ----\n","class LSH:\n","  \"\"\"\n","  The following class implements methods to apply the LSH technique.\n","  \"\"\"\n","\n","  def __init__(self, nBands :int, nRows :int, signMat :SignatureMatrix, verbose :bool=False) -> None:\n","    \"\"\"\n","    Create a new object to apply the LSH technique using ``nBands`` bands,\n","    everyone composed by ``nRows`` rows.\n","    If necessary, signatures are computed using ``signMat``.\n","    \"\"\"\n","    self.B = nBands\n","    self.R = nRows\n","    self._signMat = signMat\n","    self._verb = verbose\n","\n","    if self._verb:\n","      print('New LSH:')\n","      print(' - Num bands -> ' + str(self.B))\n","      print(' - Num rows x band -> ' + str(self.R))\n","      print(\" - Threshold set: %.2f \\n\" % ((1/self.B)**(1/self.R)*100))\n","    return\n","\n","\n","  def computeBuckets(self, x :tuple[int]|str, key :str='') -> list[tuple[tuple[int,int], dict[str, str|tuple[int]]]]:\n","    \"\"\"\n","    Assign the document ``x`` to a bucket for each band.\n","\n","    Parameters\n","    ----------\n","    ``key``: string, optional\n","      ID of the passed document ``doc``. Empty string as default value.\n","    ``x``: tuple|str\n","      The document, or its signature, to be assigned to a bucuket for each band.\n","\n","    Return\n","    ----------\n","    Return a list of tuples ``y``, one for each band.\n","    Each inner tuple contains the considered band ``i`` and the bucket ``b`` to which the document was assigned,\n","    in addition to the key and the signature of the processed document.\n","    ``y``: list\n","      [((``i``,``b``),{'key':``key``,'sign':sign}), ...]\n","    \"\"\"\n","    bucks = list()\n","    if type(x) == str:\n","      x = self._signMat.computeSignature(x)\n","      sign = x['sign']\n","    else:\n","      sign = x\n","    for i in range(0, self.B):\n","      bucks.append(((i, hash(sign[self.R*i:(self.R*i)+self.R])), {'key':key, 'sign':sign}))\n","    return bucks\n","\n","\n","  def ejsAllpairs(self, x :list[dict[str, str|int]]) -> list:\n","    \"\"\"\n","    Compute the ejs index for all the pairs of documents assigned to the same bucket of a band.\n","    The index is computed using the method ``.ejs(...)`` of the signature matrix passed to the constructor.\n","\n","    Parameters\n","    ----------\n","    ``x``: list\n","      It is a list of dictionaries. Each of them must contain the key 'sign' linked to the signature\n","      and optionally the key 'key' that contains the id of the considered document.\n","      The empty string is used as default id.\n","\n","    Return\n","    ----------\n","    ``ris``: list\n","      List of dict returned by ``SignatureMatrix.ejs(...)``.\n","    \"\"\"\n","    ris = list()\n","    for i in range(0, len(x)-1):\n","      for j in range(i+1, len(x)):\n","        ris.append(sm.ejs(x[i],x[j]))\n","    return ris"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1687781528184,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"htXP2T-ihm4q","outputId":"9e0a6d57-e206-416f-9a10-80a103e90089"},"outputs":[{"name":"stdout","output_type":"stream","text":["LSH threshold set: 0.85\n","MinHashes  per doc: 50\n"]}],"source":["# ---- PARAMETERS ----\n","# Num of reviews to read\n","J = 100000\n","# Max num of distinct shingles\n","N = 250000\n","# Num of bands\n","B = 5\n","# Num rows per bands\n","R = 10\n","# Func used to compute shingles\n","F = nonStopWords\n","# Len of K-grams (necessary only if F == kgrams)\n","K = 4\n","\n","# Required JS of docs to have at least a 50% chance of being a candidate pair (threshold)\n","T = ((1/B)**(1/R))\n","# MinHashes computed per doc\n","M = B * R\n","\n","print('LSH threshold set: %.2f' % T)\n","print('MinHashes  per doc: ' + str(M))\n","\n","# ---- FILES PATH ----\n","pathZipDs = '/content/yelp-dataset.zip'\n","pathDs = '/content/yelp_academic_dataset_review.json'"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1687781528184,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"3GCrKu8CzxAT","outputId":"72b17eba-7115-43ec-87f1-838bedd2262e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# ---- SETUP ----\n","# Set os environment variables\n","os.environ['KAGGLE_USERNAME'] = '#########'\n","os.environ['KAGGLE_KEY'] = '#########'\n","\n","# Yelp dataset\n","if not(isfile(pathZipDs)):\n","  !kaggle datasets download -d yelp-dataset/yelp-dataset\n","if not(isfile(pathDs)):\n","  !unzip /content/yelp-dataset.zip -d /content/\n","\n","# Stopwords dataset\n","nltk.download('stopwords')\n","stopWordsEn = set(nltk.corpus.stopwords.words('english'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GzTLl5fqG-tV"},"source":["**Note:** In the following cells the dataset is loaded and processed in the form of RDD, using the previously implemented methods and functions. The whole process is broken down into different cells to show the intermediate results, but it could be summerized in a few lines of code without implementing any new function."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1687781528184,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"_EmFf3ESAllE"},"outputs":[],"source":["# Instantiate the necessary objects\n","cm = CharacteristicMatrix(N, F)\n","sm = SignatureMatrix(M, cm)\n","lsh = LSH(B,R, sm)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":14597,"status":"ok","timestamp":1687781542779,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"jVSzemVDeaOJ"},"outputs":[],"source":["# Create a new spark session\n","spark = SparkSession.builder.appName('LSH').master('local[*]').getOrCreate()\n","# Get spark context\n","sc = spark.sparkContext\n","\n","# Dataframe schema\n","schema = tp.StructType().add('review_id', tp.StringType(), False).add('text', tp.StringType(), False)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75518,"status":"ok","timestamp":1687781618281,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"luPyK2mI-Euc","outputId":"b877a162-68f2-478b-8bf0-b0b68ac08143"},"outputs":[{"data":{"text/plain":["Row(review_id='KU_O5udG6zpxOg-VcAEodg', text=\"If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \\n\\nThe food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Read json file (J lines and only the fields in schema)\n","rawDataRDD = spark.read.schema(schema).json(pathDs).limit(J).rdd\n","rawDataRDD.first()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1687781618281,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"FRY-k2y-DUez"},"outputs":[],"source":["startTime = time.time()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9772,"status":"ok","timestamp":1687781628051,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"S5XJ8pKRuIvf","outputId":"cbf0ee57-b865-444a-e9c8-1df7c22cd624"},"outputs":[{"data":{"text/plain":["('KU_O5udG6zpxOg-VcAEodg',\n"," 'if you decide to eat here just be aware it is going to take about 2 hours from beginning to end we have tried it multiple times because i want to like it i have been to it s other locations in nj and never had a bad experience the food is good but it takes a very long time to come out the waitstaff is very young but usually pleasant we have just had too many experiences where we spent way too long waiting we usually opt for another diner or restaurant on the weekends in order to be done quicker ')"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Data preprocessing\n","dataRDD = rawDataRDD.map(lambda x: (x[0],preprocessing(x[1]))).cache()\n","dataRDD.first()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68047,"status":"ok","timestamp":1687781696089,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"tXc22xIj_Ojd","outputId":"9f25251b-93f3-4ba8-c918-25f3bf0c4b76"},"outputs":[{"data":{"text/plain":["{'key': 'KU_O5udG6zpxOg-VcAEodg',\n"," 'sign': (1003,\n","  6427,\n","  1030,\n","  1802,\n","  11336,\n","  1081,\n","  821,\n","  25,\n","  7865,\n","  13924,\n","  159,\n","  880,\n","  7935,\n","  4480,\n","  6471,\n","  27568,\n","  3025,\n","  21298,\n","  896,\n","  19652,\n","  191,\n","  7708,\n","  2070,\n","  4914,\n","  8099,\n","  2006,\n","  4415,\n","  10203,\n","  5992,\n","  2807,\n","  11109,\n","  5911,\n","  1419,\n","  4247,\n","  1943,\n","  2521,\n","  3350,\n","  393,\n","  678,\n","  9107,\n","  2383,\n","  1210,\n","  1013,\n","  4293,\n","  750,\n","  3186,\n","  9186,\n","  72,\n","  1719,\n","  717)}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Compute signature\n","signRDD = dataRDD.map(lambda x: sm.computeSignature(x[1],x[0])).cache()\n","signRDD.first()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1178,"status":"ok","timestamp":1687781697256,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"_hfIG0gJZD8H","outputId":"18ffc920-af52-49df-ac3e-05aa9237ab24"},"outputs":[{"data":{"text/plain":["((0, -8302861642663643509),\n"," {'key': 'KU_O5udG6zpxOg-VcAEodg',\n","  'sign': (1003,\n","   6427,\n","   1030,\n","   1802,\n","   11336,\n","   1081,\n","   821,\n","   25,\n","   7865,\n","   13924,\n","   159,\n","   880,\n","   7935,\n","   4480,\n","   6471,\n","   27568,\n","   3025,\n","   21298,\n","   896,\n","   19652,\n","   191,\n","   7708,\n","   2070,\n","   4914,\n","   8099,\n","   2006,\n","   4415,\n","   10203,\n","   5992,\n","   2807,\n","   11109,\n","   5911,\n","   1419,\n","   4247,\n","   1943,\n","   2521,\n","   3350,\n","   393,\n","   678,\n","   9107,\n","   2383,\n","   1210,\n","   1013,\n","   4293,\n","   750,\n","   3186,\n","   9186,\n","   72,\n","   1719,\n","   717)})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Assign every doc to a bucket for each band\n","# key: (band, bucket), value: {'key':id_doc, 'sign':sign_doc}\n","buckRDD = signRDD.flatMap(lambda x: lsh.computeBuckets(x['sign'], x['key']))\n","buckRDD.first()"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9785,"status":"ok","timestamp":1687781707039,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"leujHXecg0r4","outputId":"e728d64c-bd66-475d-d460-5fb56853f6d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Buckets with more than one doc: 353\n"]}],"source":["# Buckets that contains more than one doc\n","filteredBuck = buckRDD.map(lambda x: (x[0], 1)).reduceByKey(add).filter(lambda x: x[1]>1).keys().collect()\n","\n","print('Buckets with more than one doc: ' + str(len(filteredBuck)))"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7461,"status":"ok","timestamp":1687781714490,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"ZRC3IprC5Wh7","outputId":"56b70553-841a-4547-ea8a-4c48691e7f46"},"outputs":[{"data":{"text/plain":["[{'key': 'Sx8TMOWLNuJBWer-0pcmoA',\n","  'sign': (352,\n","   1325,\n","   53,\n","   1292,\n","   5510,\n","   4518,\n","   1691,\n","   4311,\n","   9042,\n","   1715,\n","   1418,\n","   4392,\n","   4068,\n","   1703,\n","   919,\n","   3935,\n","   3919,\n","   3605,\n","   587,\n","   8097,\n","   3437,\n","   413,\n","   5804,\n","   7888,\n","   8754,\n","   2006,\n","   2969,\n","   2349,\n","   5671,\n","   427,\n","   5050,\n","   1092,\n","   184,\n","   52,\n","   2860,\n","   5530,\n","   1537,\n","   2437,\n","   8890,\n","   5340,\n","   2604,\n","   9230,\n","   1158,\n","   3310,\n","   7055,\n","   771,\n","   3016,\n","   682,\n","   11093,\n","   4589)},\n"," {'key': 'DK-hGw3XSTJHT26yjzeG3Q',\n","  'sign': (352,\n","   1325,\n","   53,\n","   1292,\n","   5510,\n","   4518,\n","   1691,\n","   2338,\n","   9042,\n","   1715,\n","   1418,\n","   4392,\n","   4068,\n","   1703,\n","   919,\n","   3935,\n","   3919,\n","   3605,\n","   587,\n","   8097,\n","   3437,\n","   413,\n","   5804,\n","   7888,\n","   8754,\n","   2006,\n","   2969,\n","   2349,\n","   5671,\n","   427,\n","   5050,\n","   1092,\n","   184,\n","   52,\n","   2860,\n","   2654,\n","   1537,\n","   2437,\n","   8890,\n","   5340,\n","   2604,\n","   9230,\n","   1158,\n","   3310,\n","   7055,\n","   771,\n","   3016,\n","   682,\n","   11093,\n","   4589)}]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Compute candidate pairs (all pairs of docs in the same bucket in a band)\n","candRDD = buckRDD.filter(lambda x: x[0] in filteredBuck).groupByKey().map(lambda x: list(x[1]))\n","candRDD.first()"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":729,"status":"ok","timestamp":1687781715211,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"wT4IUBPe69jP","outputId":"9000f0d1-62db-4181-f903-ef377ddc0673"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of pairs: 355\n","\n"]},{"data":{"text/plain":["[{'keyA': 'Sx8TMOWLNuJBWer-0pcmoA',\n","  'keyB': 'DK-hGw3XSTJHT26yjzeG3Q',\n","  'EJS': 0.96},\n"," {'keyA': 'Sx8TMOWLNuJBWer-0pcmoA',\n","  'keyB': 'DK-hGw3XSTJHT26yjzeG3Q',\n","  'EJS': 0.96},\n"," {'keyA': 'Sx8TMOWLNuJBWer-0pcmoA',\n","  'keyB': 'DK-hGw3XSTJHT26yjzeG3Q',\n","  'EJS': 0.96},\n"," {'keyA': 'Zb_27vX8weaYyDn-_2ZhVA',\n","  'keyB': '-crxsnaKPE07GLVPvX0MMQ',\n","  'EJS': 1.0},\n"," {'keyA': 'Zb_27vX8weaYyDn-_2ZhVA',\n","  'keyB': '-crxsnaKPE07GLVPvX0MMQ',\n","  'EJS': 1.0}]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Compute EJS for all candidate pairs\n","res = candRDD.flatMap(lambda x: lsh.ejsAllpairs(x))\n","\n","print('Number of pairs: ' + str(res.count()) + \"\\n\")\n","res.take(5)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1496,"status":"ok","timestamp":1687781716705,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"nQOvZtDTcuXm","outputId":"7d7fc927-331c-4a02-af7b-6e8ef9270c57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of similar docs: 98\n","\n"]},{"data":{"text/plain":["[{'keyA': 'Sx8TMOWLNuJBWer-0pcmoA',\n","  'keyB': 'DK-hGw3XSTJHT26yjzeG3Q',\n","  'EJS': 0.96},\n"," {'keyA': 'Zb_27vX8weaYyDn-_2ZhVA',\n","  'keyB': '-crxsnaKPE07GLVPvX0MMQ',\n","  'EJS': 1.0},\n"," {'keyA': 'RZq-EioVPJpBgwbN2aFmVw',\n","  'keyB': 'RQY_P1raDRzMiYl1WtdK8w',\n","  'EJS': 0.84},\n"," {'keyA': 'RZq-EioVPJpBgwbN2aFmVw',\n","  'keyB': 'Jjby0Vu3NaCJEHRmWvOwQw',\n","  'EJS': 0.8},\n"," {'keyA': 'RQY_P1raDRzMiYl1WtdK8w',\n","  'keyB': 'Jjby0Vu3NaCJEHRmWvOwQw',\n","  'EJS': 0.96}]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Remove duplicate pairs\n","res = res.groupBy(lambda x: (x['keyA'],x['keyB'])).map(lambda x: list(x[1])[0])\n","\n","print('Number of similar docs: ' + str(res.count()) + \"\\n\")\n","res.take(5)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1687781716706,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"6K-v-_vwDkNF","outputId":"9cc421d0-93a4-4224-d895-3641fb34ade3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Execution time: 98.3564965724945\n"]}],"source":["endTime = time.time()\n","print(\"Execution time: \" + str(endTime - startTime))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JB_n3nZfbjad"},"source":["## Testing\n","---\n","**Note:** The following functions are for testing purposes only, given the high space and time complexity. So any failure is perfectly normal."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tpBtekfvO4GM"},"source":["**JS vs EJS**\n","> This cell is used to compute the average difference between the Jaccard Similarity index and its estimate. Of course, the parameters set deeply affect the resulting mean.\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3819,"status":"ok","timestamp":1687781720523,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"WjYwHm3c5_B5","outputId":"df1b361c-7424-4934-bca5-c7bdd30d894d"},"outputs":[{"data":{"text/plain":["0.027229965953625774"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["lst = res.collect()\n","txt = dataRDD.collectAsMap()\n","delta = 0\n","for i in lst:\n","  idD1 = i['keyA']\n","  idD2 = i['keyB']\n","\n","  shinD1 = cm.computeCharVect(txt[idD1])['shin']\n","  shinD2 = cm.computeCharVect(txt[idD2])['shin']\n","\n","  ejs = i['EJS']\n","  try:\n","    js = cm.js({'key':idD1, 'shin':shinD1}, {'key':idD2, 'shin':shinD2})['JS']\n","  except ZeroDivisionError:\n","    js = 0\n","\n","  delta += abs(ejs-js)\n","\n","delta/len(lst)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MNW0JqZZQJAT"},"source":["**A Simple Comparison**\n","> The following cell computes the estimated Jaccard distance using objects and methods from the *ml.feature* module of PySpark. The code is basically from the [official documentation](https://spark.apache.org/docs/2.2.3/ml-features.html#minhash-for-jaccard-distance)."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1687781720523,"user":{"displayName":"Matteo Onger","userId":"14345067660125991616"},"user_tz":-120},"id":"TBVevAjBbpgr","outputId":"5c8f5779-c7ca-4ffc-bbe6-bf2aa7fc6248"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfrom pyspark.ml.feature import MinHashLSH\\nfrom pyspark.ml.linalg import Vectors\\nfrom pyspark.sql.functions import col\\n\\nrow = signRDD.map(lambda x: (x[\\'key\\'],Vectors.sparse(250000, {v:1 for v in x[\\'sign\\']})))\\ndfA = spark.createDataFrame(row, [\"id\", \"features\"])\\n\\n\\nmh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\\nmodel = mh.fit(dfA)\\n\\nprint(\"The hashed dataset where hashed values are stored in the column \\'hashes\\':\")\\nmodel.transform(dfA).show()\\n\\nprint(\"Approximately joining dfA and dfB on distance smaller than 0.2:\")\\nmodel.approxSimilarityJoin(dfA, dfA, 0.2, distCol=\"JaccardDistance\")    .select(col(\"datasetA.id\").alias(\"idA\"),\\n            col(\"datasetB.id\").alias(\"idB\"),\\n            col(\"JaccardDistance\")).show()\\n'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","from pyspark.ml.feature import MinHashLSH\n","from pyspark.ml.linalg import Vectors\n","from pyspark.sql.functions import col\n","\n","row = signRDD.map(lambda x: (x['key'],Vectors.sparse(250000, {v:1 for v in x['sign']})))\n","dfA = spark.createDataFrame(row, [\"id\", \"features\"])\n","\n","\n","mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n","model = mh.fit(dfA)\n","\n","print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n","model.transform(dfA).show()\n","\n","print(\"Approximately joining dfA and dfB on distance smaller than 0.2:\")\n","model.approxSimilarityJoin(dfA, dfA, 0.2, distCol=\"JaccardDistance\")\\\n","    .select(col(\"datasetA.id\").alias(\"idA\"),\n","            col(\"datasetB.id\").alias(\"idB\"),\n","            col(\"JaccardDistance\")).show()\n","'''"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMJrsy692Pwp0ylyHApFRWJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
